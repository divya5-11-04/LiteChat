# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CxlsWui2C8nexplH3G5uvwosvuwIKKu
"""

!pip install -q transformers accelerate gradio torch

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import gradio as gr

MODEL_ID = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(MODEL_ID)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=150,
    temperature=0.7,
    do_sample=True,
)

def chat_fn(message, history):
    # Build a minimal chat-style prompt
    chat_prompt = ""
    for user, bot in history:
        chat_prompt += f"<|user|>\n{user}\n<|assistant|>\n{bot}\n"
    chat_prompt += f"<|user|>\n{message}\n<|assistant|>\n"

    output = pipe(chat_prompt)[0]["generated_text"]
    reply = output.split("<|assistant|>")[-1].strip()
    return reply

gr.ChatInterface(
    fn=chat_fn,
    title="Lightweight Chatbot (TinyLlama 1.1B Chat)",
    description="Free, small, and runs smoothly on Colab.",
).launch(share=True)